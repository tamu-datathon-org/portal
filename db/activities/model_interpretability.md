---
name: "Model Interpretability"
id: model_interpretability
startTime: June 20, 2020 10:30:00
endTime: June 20, 2020 11:30:00
mediaType: embed_url
mediaLink: https://www.youtube.com/embed/O4w0cE2rmTk
thumbnail: https://tamudatathon.com/portal/db/img/ds101.png
presenter: Sheel Dey
presenterAbout: Sheel is a Ph.D. student in computer science at Texas A&M University, working at the intersection of reinforcement learning and robotics. He has won the TAMIDS data science competitions (grad division) in 2019 and 2020.
presenterSocials:
  - type: Blog
    link: https://sheelabhadra.github.io/projects/
  - type: LinkedIn
    link: https://www.linkedin.com/in/sheelabhadra/
priority: 7
relatedActivities:
  - data_science_202
  - data_science_303
  - data_science_404
---
In this session on model interpretability, we will do a deep dive into ways to analyze why machine learning models make certain decisions. But why is model interpretability so important? Imagine that the CDC develops an antiviral drug that is speculated to be a cure for the Covid-19 virus. Based on a few trials, the CDC develops a predictive model that can correctly predict the success of the drug on a new patient 90 percent of the time. But, is just knowing the success rate enough to trust the model without caring about why it made a certain decision? What if the drug has serious negative side-effects on people with heart diseases? In this session, we will cover a few of the necessary tools such as feature importance plots, partial dependence plots, and tree interpreters to answer such questions. We will walk through the code associated with these concepts on a dataset from Kaggle.
