---
name: "Model Interpretability"
id: model_interpretability
startTime: July 25, 2020 19:00:00-500
endTime: July 25, 2020 20:00:00-500
duration: 60
mediaType: meeting_url
mediaLink: https://zoom.us/join/zoomRoomIdGoesHere
thumbnail: https://mcusercontent.com/36d73585139760aa245837bb2/images/907ef44b-9d6c-478c-baa4-5ef79cbb9901.jpeg
presenter: Sheel Dey
presenterAbout: Sheel is a Ph.D. student in computer science at Texas A&M University, working at the intersection of reinforcement learning and robotics. He has won the TAMIDS data science competitions (grad division) in 2019 and 2020.
presenterSocials:
  - type: Sheel's Blog
    link: https://sheelabhadra.github.io/projects/
  - type: Sheel's LinkedIn
    link: https://www.linkedin.com/in/sheelabhadra/
priority: 7
relatedActivities:
  - data_science_202
  - data_science_303
  - data_science_404
---
<div class="embed-responsive embed-responsive-16by9 mb-3">
<iframe src="https://www.youtube.com/embed/O4w0cE2rmTk" frameBorder="0" allowfullscreen></iframe>
</div>

In this session on model interpretability, we will do a deep dive into ways to analyze why machine learning models make certain decisions.

But why is model interpretability so important? Imagine that the CDC develops an antiviral drug that is speculated to be a cure for the Covid-19 virus. Based on a few trials, the CDC develops a predictive model that can correctly predict the success of the drug on a new patient 90 percent of the time.

But, is just knowing the success rate enough to trust the model without caring about why it made a certain decision? What if the drug has serious negative side-effects on people with heart diseases?

In this session, we will cover a few of the necessary tools such as feature importance plots, partial dependence plots, and tree interpreters to answer such questions. We will walk through the code associated with these concepts on a dataset from Kaggle.
